{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb788ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing, make_friedman1\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdde7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        num_reductions=2,\n",
    "        min_lr=1e-6,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.monitor = monitor\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.num_reductions = num_reductions\n",
    "        self.min_lr = min_lr\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.wait = 0\n",
    "        self.best = float(\"inf\")\n",
    "        self.reduction_count = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\"LR ------>\", float(tf.keras.backend.get_value(self.model.optimizer.learning_rate)))\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None and self.monitor != \"loss\":\n",
    "            current = logs.get(\"loss\")\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f\"\\nEpoch {epoch + 1}: \"\n",
    "                    f\"Metric '{self.monitor}' not found. \"\n",
    "                    f\"Falling back to 'loss'.\"\n",
    "                )\n",
    "\n",
    "        if current is None:\n",
    "            return\n",
    "\n",
    "        if current < self.best:\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            if self.restore_best_weights:\n",
    "                self.best_weights = copy.deepcopy(self.model.get_weights())\n",
    "            return\n",
    "\n",
    "        self.wait += 1\n",
    "\n",
    "        if self.wait < self.patience:\n",
    "            return\n",
    "\n",
    "        self.wait = 0\n",
    "\n",
    "        if self.reduction_count < self.num_reductions:\n",
    "            lr = self.model.optimizer.learning_rate\n",
    "            if hasattr(lr, \"assign\"):\n",
    "                old_lr = float(tf.keras.backend.get_value(lr))\n",
    "                new_lr = max(old_lr * self.factor, self.min_lr)\n",
    "                lr.assign(new_lr)\n",
    "\n",
    "            # Case 2: learning_rate is a float / int\n",
    "            elif isinstance(lr, (float, int)):\n",
    "                old_lr = float(lr)\n",
    "                new_lr = max(old_lr * self.factor, self.min_lr)\n",
    "                self.model.optimizer.learning_rate = new_lr\n",
    "\n",
    "            # Case 3: learning rate schedule → cannot be reduced\n",
    "            elif isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "                if self.verbose:\n",
    "                    print(\n",
    "                        \"\\nLearning rate is a schedule; \"\n",
    "                        \"ReduceLROnPlateau behavior is disabled.\"\n",
    "                    )\n",
    "                return\n",
    "\n",
    "            else:\n",
    "                raise TypeError(\n",
    "                    f\"Unsupported learning_rate type: {type(lr)}\"\n",
    "                )\n",
    "\n",
    "            self.reduction_count += 1\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f\"\\nEpoch {epoch + 1}: \"\n",
    "                    f\"ReduceLROnPlateau {self.reduction_count}/{self.num_reductions} \"\n",
    "                    f\"— LR {old_lr:.3e} → {new_lr:.3e}\"\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f\"\\nEpoch {epoch + 1}: \"\n",
    "                    f\"Early stopping triggered after {self.num_reductions} LR reductions.\"\n",
    "                )\n",
    "\n",
    "            if self.restore_best_weights and self.best_weights is not None:\n",
    "                if self.verbose:\n",
    "                    print(\"Restoring best model weights.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd9de6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5ac3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f26f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = make_friedman1(n_samples=5000, n_features=10, noise=1.0, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a21a4060",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e1bdeaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation='relu', kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation='relu', kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation='relu', kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "800d997d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20640, 8), (20640,))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "585fa0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZERS = {\n",
    "    'adam': tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    'adamw': tf.keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-4),\n",
    "    'nadam': tf.keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "    'rmsprop': tf.keras.optimizers.RMSprop(learning_rate=5e-4),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e24dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = OPTIMIZERS['nadam'] # Epoch 32/200 loss: 1.0644 - mae: 0.8193 - val_loss: 1.0346 - val_mae: 0.8113\n",
    "optimizer = OPTIMIZERS['adam'] # Epoch 29/200 loss: 1.2096 - mae: 0.8827 - val_loss: 1.0778 - val_mae: 0.8260\n",
    "# optimizer = OPTIMIZERS['adamw'] # Epoch 39/200 loss: 1.0883 - mae: 0.8289 - val_loss: 1.1170 - val_mae: 0.8352\n",
    "# optimizer = OPTIMIZERS['rmsprop'] # Epoch 37/200 loss: 1.3758 - mae: 0.9286 - val_loss: 1.8918 - val_mae: 1.1354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7f5bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f687b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = CustomScheduler(patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d449a779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m593/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 769us/step - loss: 1.0061 - mae: 0.7297LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.9782 - mae: 0.7186 - val_loss: 0.5351 - val_mae: 0.5063\n",
      "Epoch 2/200\n",
      "\u001b[1m613/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 742us/step - loss: 0.5106 - mae: 0.5172LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 924us/step - loss: 0.5102 - mae: 0.5168 - val_loss: 0.5004 - val_mae: 0.4771\n",
      "Epoch 3/200\n",
      "\u001b[1m596/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 0.4737 - mae: 0.4896LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 913us/step - loss: 0.4730 - mae: 0.4894 - val_loss: 0.4525 - val_mae: 0.4533\n",
      "Epoch 4/200\n",
      "\u001b[1m594/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 0.4412 - mae: 0.4753LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 902us/step - loss: 0.4418 - mae: 0.4753 - val_loss: 0.4549 - val_mae: 0.4475\n",
      "Epoch 5/200\n",
      "\u001b[1m633/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 0.4187 - mae: 0.4566LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 868us/step - loss: 0.4190 - mae: 0.4568 - val_loss: 0.4435 - val_mae: 0.4454\n",
      "Epoch 6/200\n",
      "\u001b[1m596/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 0.4356 - mae: 0.4652LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.4349 - mae: 0.4648 - val_loss: 0.4793 - val_mae: 0.4640\n",
      "Epoch 7/200\n",
      "\u001b[1m628/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 0.4249 - mae: 0.4559LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 876us/step - loss: 0.4246 - mae: 0.4558 - val_loss: 0.4311 - val_mae: 0.4331\n",
      "Epoch 8/200\n",
      "\u001b[1m610/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 0.3977 - mae: 0.4409LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 894us/step - loss: 0.3979 - mae: 0.4410 - val_loss: 0.4086 - val_mae: 0.4311\n",
      "Epoch 9/200\n",
      "\u001b[1m599/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 764us/step - loss: 0.4043 - mae: 0.4458LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 912us/step - loss: 0.4043 - mae: 0.4457 - val_loss: 0.3920 - val_mae: 0.4271\n",
      "Epoch 10/200\n",
      "\u001b[1m633/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 0.4042 - mae: 0.4434LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 944us/step - loss: 0.4041 - mae: 0.4434 - val_loss: 0.3960 - val_mae: 0.4241\n",
      "Epoch 11/200\n",
      "\u001b[1m589/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 0.4084 - mae: 0.4473LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau 1/2 — LR 1.000e-03 → 5.000e-04\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 989us/step - loss: 0.4070 - mae: 0.4465 - val_loss: 0.4268 - val_mae: 0.4835\n",
      "Epoch 12/200\n",
      "\u001b[1m634/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 0.3774 - mae: 0.4264LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 968us/step - loss: 0.3772 - mae: 0.4263 - val_loss: 0.3813 - val_mae: 0.4129\n",
      "Epoch 13/200\n",
      "\u001b[1m633/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 889us/step - loss: 0.3622 - mae: 0.4185LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.3623 - mae: 0.4185 - val_loss: 0.3656 - val_mae: 0.4163\n",
      "Epoch 14/200\n",
      "\u001b[1m638/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 0.3609 - mae: 0.4151LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 959us/step - loss: 0.3609 - mae: 0.4151 - val_loss: 0.3588 - val_mae: 0.4079\n",
      "Epoch 15/200\n",
      "\u001b[1m625/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 0.3637 - mae: 0.4192LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 884us/step - loss: 0.3635 - mae: 0.4191 - val_loss: 0.3556 - val_mae: 0.4092\n",
      "Epoch 16/200\n",
      "\u001b[1m625/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 0.3641 - mae: 0.4212LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 869us/step - loss: 0.3638 - mae: 0.4210 - val_loss: 0.3515 - val_mae: 0.4105\n",
      "Epoch 17/200\n",
      "\u001b[1m585/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 0.3592 - mae: 0.4138LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 918us/step - loss: 0.3586 - mae: 0.4137 - val_loss: 0.3440 - val_mae: 0.4031\n",
      "Epoch 18/200\n",
      "\u001b[1m644/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 0.3534 - mae: 0.4100LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.3534 - mae: 0.4100 - val_loss: 0.3685 - val_mae: 0.4320\n",
      "Epoch 19/200\n",
      "\u001b[1m594/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 0.3391 - mae: 0.4042LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau 2/2 — LR 5.000e-04 → 2.500e-04\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 944us/step - loss: 0.3396 - mae: 0.4044 - val_loss: 0.3479 - val_mae: 0.4052\n",
      "Epoch 20/200\n",
      "\u001b[1m609/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 0.3238 - mae: 0.3948LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 904us/step - loss: 0.3241 - mae: 0.3948 - val_loss: 0.3532 - val_mae: 0.3914\n",
      "Epoch 21/200\n",
      "\u001b[1m631/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 0.3292 - mae: 0.3956LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 881us/step - loss: 0.3292 - mae: 0.3956 - val_loss: 0.3332 - val_mae: 0.3864\n",
      "Epoch 22/200\n",
      "\u001b[1m627/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 0.3230 - mae: 0.3928LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.3231 - mae: 0.3929 - val_loss: 0.3323 - val_mae: 0.3985\n",
      "Epoch 23/200\n",
      "\u001b[1m583/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 784us/step - loss: 0.3204 - mae: 0.3912LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 927us/step - loss: 0.3207 - mae: 0.3912 - val_loss: 0.3431 - val_mae: 0.4218\n",
      "Epoch 24/200\n",
      "\u001b[1m635/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 0.3152 - mae: 0.3883LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 889us/step - loss: 0.3153 - mae: 0.3884 - val_loss: 0.3274 - val_mae: 0.3997\n",
      "Epoch 25/200\n",
      "\u001b[1m577/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 0.3154 - mae: 0.3873LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 928us/step - loss: 0.3159 - mae: 0.3877 - val_loss: 0.3224 - val_mae: 0.3839\n",
      "Epoch 26/200\n",
      "\u001b[1m602/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 0.3167 - mae: 0.3887LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 916us/step - loss: 0.3168 - mae: 0.3887 - val_loss: 0.3223 - val_mae: 0.3811\n",
      "Epoch 27/200\n",
      "\u001b[1m609/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 0.3063 - mae: 0.3840LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 903us/step - loss: 0.3067 - mae: 0.3842 - val_loss: 0.3164 - val_mae: 0.3855\n",
      "Epoch 28/200\n",
      "\u001b[1m605/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 926us/step - loss: 0.3105 - mae: 0.3828LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.3106 - mae: 0.3830 - val_loss: 0.3162 - val_mae: 0.3821\n",
      "Epoch 29/200\n",
      "\u001b[1m583/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 0.3136 - mae: 0.3858LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 922us/step - loss: 0.3133 - mae: 0.3858 - val_loss: 0.3182 - val_mae: 0.3804\n",
      "Epoch 30/200\n",
      "\u001b[1m612/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 0.3052 - mae: 0.3811LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.3054 - mae: 0.3812 - val_loss: 0.3129 - val_mae: 0.3811\n",
      "Epoch 31/200\n",
      "\u001b[1m636/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.3089 - mae: 0.3843LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.3088 - mae: 0.3843 - val_loss: 0.3093 - val_mae: 0.3756\n",
      "Epoch 32/200\n",
      "\u001b[1m603/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 0.3112 - mae: 0.3834LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 903us/step - loss: 0.3108 - mae: 0.3833 - val_loss: 0.3087 - val_mae: 0.3753\n",
      "Epoch 33/200\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 0.2954 - mae: 0.3767LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 852us/step - loss: 0.2954 - mae: 0.3767 - val_loss: 0.3240 - val_mae: 0.3748\n",
      "Epoch 34/200\n",
      "\u001b[1m627/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 0.3008 - mae: 0.3778LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 919us/step - loss: 0.3008 - mae: 0.3778 - val_loss: 0.3053 - val_mae: 0.3818\n",
      "Epoch 35/200\n",
      "\u001b[1m587/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 0.2995 - mae: 0.3794LR ------> <KerasVariable shape=(), dtype=float32, path=adam/learning_rate>\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 996us/step - loss: 0.2995 - mae: 0.3793 - val_loss: 0.3063 - val_mae: 0.3784\n",
      "Epoch 36/200\n",
      "\u001b[1m504/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2968 - mae: 0.3775"
     ]
    }
   ],
   "source": [
    "# es = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "model.fit(X, y, validation_data=(X_test, y_test), epochs=200, callbacks=[es])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
