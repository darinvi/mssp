{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb788ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing, make_friedman1\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdde7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        num_reductions=2,\n",
    "        min_lr=1e-6,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.monitor = monitor\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.num_reductions = num_reductions\n",
    "        self.min_lr = min_lr\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.wait = 0\n",
    "        self.best = float(\"inf\")\n",
    "        self.reduction_count = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\"LR ------>\", float(tf.keras.backend.get_value(self.model.optimizer.learning_rate)))\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None and self.monitor != \"loss\":\n",
    "            current = logs.get(\"loss\")\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f\"\\nEpoch {epoch + 1}: \"\n",
    "                    f\"Metric '{self.monitor}' not found. \"\n",
    "                    f\"Falling back to 'loss'.\"\n",
    "                )\n",
    "\n",
    "        if current is None:\n",
    "            return\n",
    "\n",
    "        if current < self.best:\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            if self.restore_best_weights:\n",
    "                self.best_weights = copy.deepcopy(self.model.get_weights())\n",
    "            return\n",
    "\n",
    "        self.wait += 1\n",
    "\n",
    "        if self.wait < self.patience:\n",
    "            return\n",
    "\n",
    "        self.wait = 0\n",
    "\n",
    "        if self.reduction_count < self.num_reductions:\n",
    "            lr = self.model.optimizer.learning_rate\n",
    "            if hasattr(lr, \"assign\"):\n",
    "                old_lr = float(tf.keras.backend.get_value(lr))\n",
    "                new_lr = max(old_lr * self.factor, self.min_lr)\n",
    "                lr.assign(new_lr)\n",
    "\n",
    "            # Case 2: learning_rate is a float / int\n",
    "            elif isinstance(lr, (float, int)):\n",
    "                old_lr = float(lr)\n",
    "                new_lr = max(old_lr * self.factor, self.min_lr)\n",
    "                self.model.optimizer.learning_rate = new_lr\n",
    "\n",
    "            # Case 3: learning rate schedule → cannot be reduced\n",
    "            elif isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "                if self.verbose:\n",
    "                    print(\n",
    "                        \"\\nLearning rate is a schedule; \"\n",
    "                        \"ReduceLROnPlateau behavior is disabled.\"\n",
    "                    )\n",
    "                return\n",
    "\n",
    "            else:\n",
    "                raise TypeError(\n",
    "                    f\"Unsupported learning_rate type: {type(lr)}\"\n",
    "                )\n",
    "\n",
    "            self.reduction_count += 1\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f\"\\nEpoch {epoch + 1}: \"\n",
    "                    f\"ReduceLROnPlateau {self.reduction_count}/{self.num_reductions} \"\n",
    "                    f\"— LR {old_lr:.3e} → {new_lr:.3e}\"\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f\"\\nEpoch {epoch + 1}: \"\n",
    "                    f\"Early stopping triggered after {self.num_reductions} LR reductions.\"\n",
    "                )\n",
    "\n",
    "            if self.restore_best_weights and self.best_weights is not None:\n",
    "                if self.verbose:\n",
    "                    print(\"Restoring best model weights.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd9de6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e80a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X)\n",
    "df['y'] = y\n",
    "df.columns = [f\"x_{c}\" if c != 'y' else c for c in df.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57245870",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['y'])\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413a5c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        4.526\n",
       "1        3.585\n",
       "2        3.521\n",
       "3        3.413\n",
       "4        3.422\n",
       "         ...  \n",
       "20635    0.781\n",
       "20636    0.771\n",
       "20637    0.923\n",
       "20638    0.847\n",
       "20639    0.894\n",
       "Name: y, Length: 20640, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08e2ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = make_friedman1(n_samples=5000, n_features=10, noise=1.0, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a4060",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bdeaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation='relu', kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation='relu', kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation='relu', kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585fa0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZERS = {\n",
    "    'adam': tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    'adamw': tf.keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-4),\n",
    "    'nadam': tf.keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "    'rmsprop': tf.keras.optimizers.RMSprop(learning_rate=5e-4),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e24dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = OPTIMIZERS['nadam'] # Epoch 32/200 loss: 1.0644 - mae: 0.8193 - val_loss: 1.0346 - val_mae: 0.8113\n",
    "optimizer = OPTIMIZERS['adam'] # Epoch 29/200 loss: 1.2096 - mae: 0.8827 - val_loss: 1.0778 - val_mae: 0.8260\n",
    "# optimizer = OPTIMIZERS['adamw'] # Epoch 39/200 loss: 1.0883 - mae: 0.8289 - val_loss: 1.1170 - val_mae: 0.8352\n",
    "# optimizer = OPTIMIZERS['rmsprop'] # Epoch 37/200 loss: 1.3758 - mae: 0.9286 - val_loss: 1.8918 - val_mae: 1.1354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f5bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f687b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = CustomScheduler(patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d449a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# es = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "model.fit(X, y, validation_data=(X_test, y_test), epochs=200, callbacks=[es])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
